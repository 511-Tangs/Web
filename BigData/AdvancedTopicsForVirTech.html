<h3>Devuan Jessie released on May 25, 2017</h3>

<A href="https://www.theregister.co.uk/2017/05/26/devuan_1_0_long_term_support_released/"
 target="_b">Devuan 1.0.0</a> "System Upgrade" used to be a daily ritual.  Nowadays, 
we seldom do it with the fear of further damaging our system, since you wonder what 
other unwanted RH features might be introduced to our system during system upgrade.

<h3>Advanced Topics for Virtualization Technology</h3>

<OL>
  <LI><h4>Three Types of Rootfilesystems</h4>

<OL>
  <LI><a href="file:///src3/NetLab/Nets/MakingMinimalUmlRfs.sh" target="_b">Minimalist 
      UmlRfs</a>

    <h4>Note: a minimalist file system</h4>   

    <P> UmlRfs:  Created via the shell script: <b>MakingMinimalUmlRfs.sh</b>.  Using 
    this single UmlRfs, we may simulate a rather complicated network, (with say, a 
    dozen of VMs), with minimum system resource.
     

<PRE>
$ du -m UmlRfs
79	UmlRfs
$ cat /tmp/startUML
screen -S hostC -d -m linux.uml mem=128M rootfstype=hostfs \
rootflags=/home/hsu/UmlRfs rw hostname=hostC umid=hostC \
con=xterm con0=fd:0,fd:1 &
$ /tmp/startUML
$ screen -ls
There is a screen on:
	2928.hostC	(05/27/2017 09:54:03 PM)	(Detached)
$ ps l -C linux.uml
F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
0  1000  2929  2928  20   0 151264 15068 -      Ss+  pts/3      0:01 linux.uml m
1  1000  2934  2929  20   0 151264 15068 -      S+   pts/3      0:00 linux.uml m
1  1000  2935  2929  20   0 151264 15068 unix_s S+   pts/3      0:00 linux.uml m
1  1000  2936  2929  20   0 151264 15068 SyS_po S+   pts/3      0:00 linux.uml m
1  1000  2938  2929  20   0  19248   984 -      t+   pts/3      0:00 linux.uml m
1  1000  2939  2929  20   0  19344   532 -      t+   pts/3      0:00 linux.uml m
1  1000  2957  2929  20   0  19256   596 -      t+   pts/3      0:00 linux.uml m
$ screen -R 2928.hostC
# Press "enter" key to get the system prompt "#", you can issue linux commands (provides 
# by busybox) after the prompt. The next few commands are executed in hostC
/ # hostname
hostC
/ # ls -l /bin 
/ # ls -l /bin/halt
lrwxrwxrwx    1 1000     1000             7 Mar 23  2015 /bin/halt -> busybox
/ # hostname
hostC
/ # df
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/root             19092180   5094604  13004708  28% /
devtmpfs                 60284         0     60284   0% /dev
none                  14286320   1023348  12514224   8% /var/tmp/TcpLog
################################################################
# The next command is executed in the physical host
# $  df /home
# Filesystem     1K-blocks    Used Available Use% Mounted on
# /dev/sda8       19092180 5094616  13004696  29% /home
################################################################
/ # halt -f 
[screen is terminating]
$ ps l -C linux.uml
F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
$ ps l -C screen
F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
</PRE>
  <LI><a href="./vdev-test.html#UmlTemplateViaDevuan" target="_b">Uml Rootfs via 
      Devuan</a>

      <P> Nowadays, our UML Template is based on devuan. <b>Shortcoming:</b> The network 
          bandwidth of this devuan organization is very limited.  Install one template, 
          everybody shares it by copying the sucessfully installed one.  
          <b>Advantage:</b> Very reliable. Our sd (software development) VMs are based 
          on it.  So far, these VMs never let us down, yet.

  <LI><a href="" target="_b">Kvm-based Rootfs</a>

      <P>

  <LI><a href="https://github.com/avpatel/xvisor-next/" target="_b">Xvisor</a>


      <P> If possible, we are and will be moving away from KVM to Xvisor, a new 
          generation of Virtual Technology.  KVM is completely dominated by Red 
          Hat, a not trustworthy company.

</OL>

      <P> <b>Lesson Learned</b>: 

    <UL>
      <LI> The first part of this semester: Maintaining the UML 
          based VMs which offer Linux Environments for students in Open System 1 
          class. Also, we built and maintained the rootfs DevUan-UltraLight.ext4 for 
          UML-based VM.  Both the root file systems of these two VMs were built upon  
          the DevUan Linux distribution. Both systems are very stable and reliable. 
          Also, we have been using our debian mirror for software upgrade without any 
          glitch, so far.  The job of resizing root file system is rather easy and
          Bulletproof.
      <LI> Debian: The Universal Operating System

      <P> Two and half years ago, I really agreed with this claim.  Today, it is nothing
        but a lousy Red Hat follower. Check this article: <a target="_b" 
        href="./NoteOnSoftwarePorting.html">Notes On Software Porting</a>, you know why.
        Our sd (software develpoment) VM is a rather useful and neat VM created to 
        solve our Software Porting problem.  Also, I believe, with a few more libraries 
        installed in it, we can resolve the UML compilation obstacle we are facing today 
        in the Debian environment.
      <LI> The second part of this semester: We have been attempting to create a 
        reliable KVM-based root filesystem.

      <P> In the first week, we quickly and successfully built the root filesystem. But, 
        after adding the <b>openssh-server</b> package to it, our <b>libc.so</b> was 
        upgraded to libc-2.24.so, a brain damaged one.  And our <b>init</b> process was 
        impersonated by <b>systemd</b>. Worst of all, we only got 244M storage space 
        left. We add 1GB space to it, and use <b>gp</b> VM to resize its file system. We
        then <b>sudo apt-get update; sudo apt-get upgrade</b> the system and add a dozen 
        or so packages to it. On the fourth week, Sorry, we booted our VM in the system 
        repairing mode: A system prompt "#" issued by <b>/bin/sh</b>.  Since there is no 
        network, we were not able to repair it at all.
         

      <P> At home, (04/30/2017), I investigated all the software installed in the root 
        filesystem and decided to <a href="./Dev1Installation.html#PurgeUselessPackages" 
        target="_b">Purge Useless Packages</a>.  Based on this modified root filesystem,
        I successfully repaired it.  Our newly adopted <b>Dev1-Mini.img</b> rootfs is 
        not enlarged.  Still, we have almost 1GB storge space left.         

      <P> In order to prevent our ethercard from being renamed, we have the two files: 

<PRE>
$ cat recover70rules 
#! /bin/bash

if [ $# -gt 0 ]
  then echo "This script empties the harmful /etc/udev/rules.d/70-persistent-net.rules file"
elif  [ ! -f Empty70NetFile ]
  then echo "Empty70NetFile does not exist!"
       exit 1
elif  [ $EUID -ne 0 ]
  then echo "Need to be Super User"
       exit 2
else 
  cp Empty70NetFile /etc/udev/rules.d/70-persistent-net.rules
fi
$ diff /etc/default/grub /etc/default/grub.orig
10c10
< GRUB_CMDLINE_LINUX="net.ifnames=0 biosdevname=0"
---
> GRUB_CMDLINE_LINUX=""
</PRE>
  </UL>

<LI> <h4>Root Filesystem Resizing</h4>


      <UL>
         <LI> UML Root Filesystem: <b>ResizeRootFS</b> in /src3/UML
         <LI> KVM Root Filesystem: <b>start-Gparted-6-resizing</b> in /src4/KVM/bin
      </UL>

      <P> The significance: If our root filesystem templates are tested throughly, and 
          we feel that they are stable and reliable.  We can always take any of them 
          to clone our VMs safely and quickly.  But, very often, we need more storage 
          space for additional special purpose software packages.


        

  <LI><b>SDS</b>: Software Defined Storage and <b>SDN</b>: Software Defined Network

      <P> The more suitable solution will be storage virtualization. Along this line, 
          we have been investigating and prototyping 
          <a href="http://amdm/LinuxRef/CephAndVirtualStorage/" 
          target="_b">Ceph: Virtual Storage Solution</a>, Also see: <a target="_b" 
href="http://amdm/LinuxRef/CephAndVirtualStorage/AnIntegratedVirtualStorageCluster.html">An 
          Integrated Virtual Storage Cluster</a>, a storage virtualization tutorial.

      <P> For a BigData cloud, using <b>ceph</b>, we can resolve our storage 
          virtualization requirement.  These ceph related VMs, three types: <b>osd</b>, 
          object store daemon, <b>mon</b>, monitor, <b>mds</b>, metadata server, 
          consist of the storage cluster.  We also need another cluster for data 
          analytics purpose.  You can image that with these two clusters of VMs 
          interact together, the tremendous network traffics generated demands our 
          network to be virtualized, via so called SDN, software defined network, 
        <a href="http://amdm/LinuxRef/CephAndVirtualStorage/NetworkVirtualization.html" 
           target="_b">Network Virtualization</a>

      <P> The most important feature of a bigdata cloud, I believe, would be horizontally
          scalable.   We can start it small, when more storage and computation power 
          are needed, we can simply add more units (or raw machines) to it.  A failure 
          unit can easily be replaced by another unit or by simply unplug it from the 
          cloud and let the rest VMs take over its workload.
          

  <LI> <a href="http://amdm/LectureNotes/Diaries/VirTech-2017.html#Cow" target="_b">COW: 
       Copy On Write, also known as Overlay Technique</a> 

      <P> Not suitable for VMs in a high availability cluster. It's OK for VMs in
          high performance  or distributed computing clusters.

      <P> Also recall the failure of VM booting after software upgrading of the 
          template Root Filesystem after creation of our overlay image.

<PRE>
  $ cd /src4/KVM/Resize 
  $ ls -l 
  $ qemu-img create -b Debian-Gparted.img -f qcow2 gp2.ovl 
  $ ls -l *ovl 
-rw-r--r-- 1 hsu hsu 196656 Jun  5 08:24 gp2.ovl
  $ fdisk -l  gp2.ovl 
Disk gp2.ovl: 192 KiB, 196608 bytes, 384 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

  # Disk gp2.ovl doesn't contain a valid partition table
  # We need to mount it via nbd (network block device). First, load the nbd module.
  $ sudo modprobe nbd max_part=8 
  # We get a few device drivers which allow us to mount filesystems in gp2.ovl
  $ ls -l /dev/nbd* 
  $ sudo qemu-nbd --connect=/dev/nbd0 gp2.ovl 
  $ sudo fdisk -l /dev/nbd0
Disk /dev/nbd0: 3 GiB, 3221225472 bytes, 6291456 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x0006bf7e

Device      Boot  Start     End Sectors  Size Id Type
/dev/nbd0p1 *      2048  999423  997376  487M 83 Linux
/dev/nbd0p2      999424 6291455 5292032  2.5G 83 Linux
  $ ls -l /dev/nbd0*
brw-rw---- 1 root disk 43, 0 Jun  5 08:28 /dev/nbd0
  # Device files /dev/nbd0p1 and /dev/nbd0p2 do not exist, yet.
  $ sudo partprobe /dev/nbd0
  $ ls -l /dev/nbd0* 
  $ sudo mount /dev/nbd0p2 /mnt/tmp 
  $ sudo mount /dev/nbd0p1 /mnt/tmp/boot 
  $ ls -l /mnt/tmp 
  $ ls -l /mnt/tmp/boot 
  # We need to edit 3 files in gp2.ovl 
  $ ls -l /mnt/tmp/etc/hostname* /mnt/tmp/etc/hosts* /mnt/tmp/etc/rc.local* 
  $ sodo cp $HOME/.Xau* /root 
  $ find /mnt/tmp/etc -name "*~" 2>/dev/null 
/mnt/tmp/etc/network/interfaces~
/mnt/tmp/etc/hostname~
/mnt/tmp/etc/hosts~
/mnt/tmp/etc/rc.local~
  $ diff /mnt/tmp/etc/hostname /mnt/tmp/etc/hostname~
1c1
< gp2
---
> gp
  $ diff /mnt/tmp/etc/hosts /mnt/tmp/etc/hosts~
2c2
< 127.0.1.1       gp2
---
> 127.0.1.1       gp
  $ diff /mnt/tmp/etc/rc.local /mnt/tmp/etc/rc.local~
19c19
< ifconfig eth0 192.168.0.247
---
> ifconfig eth0 192.168.0.248
  # Must umount /mnt/tmp/boot, first. And then /mnt/tmp.  I.e. reverse the order.
  $ sudo umount /mnt/tmp/boot /mnt/tmp 
  $ sudo qemu-nbd --disconnect /dev/nbd0 
  $ sudo rmmod nbd 
  # Now, prepare the start and stop scripts for it.  We can not use Config-Kvm script,
  # since we will not be able to mount gp2.ovl.  We get the start-* and stop-* scripts
  # from VM gp.  However, Minix3 and Hurd root filesystems are not mounted, either. 
  # It should not be too difficult to modify Config-Kvm script to take care of *ovl 
  # files.  Worth it?  That's the real question.  
  $ ls -l ../bin/*6*
-rwxr-xr-x 1 hsu hsu 1027 Mar  8 15:02 ../bin/start-gp-6
-rwxr-xr-x 1 hsu hsu 1054 Mar  8 15:02 ../bin/start-gp-6-AsDaemon
-rwxr-xr-x 1 hsu hsu 1147 Mar  8 15:02 ../bin/start-gp-6-efs
-rwxr-xr-x 1 hsu hsu 1937 May  4  2016 ../bin/start-gp-6-resizing
-rwxr-xr-x 1 hsu hsu 1791 Mar  8 15:02 ../bin/stop-gp-restore-lan-6
  $ cp ../bin/start-gp-6 ../bin/stop-gp-restore-lan-6 .
  $ mv start-gp-6 start-gp-8
  $ mv stop-gp-restore-lan-6 stop-gp-restore-lan-8
  $ ls -l st*
-rwxr-xr-x 1 hsu hsu 1006 Jun  5 08:54 start-gp-8
-rwxr-xr-x 1 hsu hsu 1027 Jun  5 08:50 start-gp-8~
-rwxr-xr-x 1 hsu hsu 1791 Jun  5 08:55 stop-gp-restore-lan-8
-rwxr-xr-x 1 hsu hsu 1791 Jun  5 08:50 stop-gp-restore-lan-8~
  # Need to globally replace string 140.120.8.120 by your own IP
  $ diff start-gp-8 start-gp-8~
12,13c12,13
< sudo tunctl -u hsu -t tap8
< sudo ifconfig tap8 140.120.8.120 netmask 255.255.255.255 up
---
> sudo tunctl -u hsu -t tap6
> sudo ifconfig tap6 140.120.8.120 netmask 255.255.255.255 up
15c15
< sudo iptables -A FORWARD --in-interface tap8 -j ACCEPT
---
> sudo iptables -A FORWARD --in-interface tap6 -j ACCEPT
17,19c17,19
< sudo sysctl net.ipv4.conf.tap8.proxy_arp=1
< sudo arp -Ds 192.168.0.247 eth0 pub
< sudo route add -host 192.168.0.247 dev tap8
---
> sudo sysctl net.ipv4.conf.tap6.proxy_arp=1
> sudo arp -Ds 192.168.0.248 eth0 pub
> sudo route add -host 192.168.0.248 dev tap6
21c21
< vde_switch -tap tap8 -mod 644 -sock=/src4/KVM/network-11982 -mgmt /src4/KVM/network-11982/vde_switch.mgmt -daemon </dev/null >/dev/null
---
> vde_switch -tap tap6 -mod 644 -sock=/src4/KVM/network-11982 -mgmt /src4/KVM/network-11982/vde_switch.mgmt -daemon </dev/null >/dev/null
23c23
< kvm -net vde,vlan=0,sock=/src4/KVM/network-11982 -net nic,vlan=0,macaddr=00:1f:d0:d8:74:43 -m 512M -monitor unix:/src4/KVM/network-11982/MonSock,server,nowait -hda gp2.ovl &
---
> kvm -net vde,vlan=0,sock=/src4/KVM/network-11982 -net nic,vlan=0,macaddr=00:1f:d0:d8:74:43 -m 512M -monitor unix:/src4/KVM/network-11982/MonSock,server,nowait -hda ../Resize/Debian-Gparted.img &
  $ diff stop-gp-restore-lan-8 stop-gp-restore-lan-8~
34c34
< ping -c 3 192.168.0.247
---
> ping -c 3 192.168.0.248
38c38
<        ssh -t hsu@192.168.0.247 'sudo init 0'
---
>        ssh -t hsu@192.168.0.248 'sudo init 0'
44c44
< sudo pkill -f "vde_switch -tap tap8 -mod 644 -sock=/src4/KVM/network-11982 -mgmt /src4/KVM/network-11982/vde_switch.mgmt"
---
> sudo pkill -f "vde_switch -tap tap6 -mod 644 -sock=/src4/KVM/network-11982 -mgmt /src4/KVM/network-11982/vde_switch.mgmt"
50,51c50,51
< sudo sysctl net.ipv4.conf.tap8.proxy_arp=0
< sudo ifconfig tap8 140.120.8.120 down
---
> sudo sysctl net.ipv4.conf.tap6.proxy_arp=0
> sudo ifconfig tap6 140.120.8.120 down
53,56c53,56
< sudo iptables -D FORWARD --in-interface tap8 -j ACCEPT
< # sudo route del -host 192.168.0.247 dev tap8
< # sudo arp -d 192.168.0.247
< sudo tunctl -d tap8
---
> sudo iptables -D FORWARD --in-interface tap6 -j ACCEPT
> # sudo route del -host 192.168.0.248 dev tap6
> # sudo arp -d 192.168.0.248
> sudo tunctl -d tap6
  $ ls -l *ovl
-rw-r--r-- 1 hsu hsu 6225920 Jun  5 08:45 gp2.ovl
  $ start-gp-8
  $ ssh -X hsu@192.168.0.247
  $ stop-gp-restore-lan-8
  $ ls -l *ovl
-rw-r--r-- 1 hsu hsu 15728640 Jun  5 09:23 gp2.ovl
</PRE>

      <P> <b>Shared objects vs Static libraries</b>

      <UL>
        <LI>static library 
        <UL>
          <LI>Pros
              <P> Only the develpoer's os need to have the library installed.
          <LI>Cons
              <P> Waste memory, maintenance nightmare.
        </UL>
        <LI>shared object library
        <UL>
          <LI>Pros
              <P>memory saving, ease of maintenance
          <LI>Cons
              <P>Every machine must have the shared object library installed, even if 
                 only one package needs it.
        </UL>
      </UL>

      <P><b>Note:</b> The shared object library is <b>not</b> that great, either.  We 
         are easily held hostages of some unethical companies, such as Red Hat.
       
  <LI> <b>qemu-system-arm</b>

       <P> Using <b>qemu-system-arm</b>, we may run raspberry pi2 virtual machine 
       in our x86_64 based physical hosts.  (We need to have 4 or more cores in 
       our motherboard to boot it successfully.) 

       <P> <b>Note: (06/02/2016)</b> Still, I can't figure out why we can boot 
         Rpi2 linux 3.18, (and Rpi2 linux 4.1, tested recently), but not Rpi2 linux 
         4.4.

       <P> This is really significant achievement.  I have created a cross-compilation 
         VM for developing software for arm based machines in x86_64 host.  We all 
         know a decent x86_64 based machine, (say, with 6 or more cores), has better 
         software development environment, faster compilation speed, etc.  If we need 
         to test the software we generated immediately, this is the solution I am 
         aiming at.

       <P> Nowadays, I am investigating the possibility of using <b>qemu-system-arm</b>
          to test odroid-xu4, <a href="http://odroid.us/mediawiki/index.php?
          title=Step-by-step_Using_qemu_to_Boot_an_Emulated_Odroid" 
          target="_b">Using qemu to Boot an Emulated Odroid</a> might be helpful.
  <LI> <b>vdev</b>

       <P> We have encountered all the difficulties imposed on us by <b>Red-Hat</b>,
           to name a couple of it: shameless <b>systemd</b> pollution, unpredictable 
           ethercard renaming, etc.  In the near future, we will be based our linux 
           systems on the ones distributed by <b>devuan</b>.  At least, we can have 
           a systemd-free system.  Our next goal is freeing ourselves from the udev
           and dbus hazardous software traps.

       <P> Our achievement:  We successfully replaced udev by vdev on our dev1 VMs.  
           Also, we freely, (without any fear or doubt), and successfully replaced 
           udev by vdev, since we knew we could afford the failure without heavy 
           penalty.  udev is a replacement for the Device File System (DevFS) starting 
           with the Linux 2.6 kernel series

      <h5>P.S.</h5>         

      <P>  In order to end our class at 3.40 PM sharp so that everybody had enough time 
       to reboot our system to sda (, the first disk), I forgot to shutdown the dev1 
       VM, the virtual machine we replaced udev by vdev.  Consequence: dev1 rootfs 
       was damaged.  No harm done! I have fixed every Rootfs of this VM in each host.   

      <P> The most severe damaged VM was the one in ac11.  The reason: A lot of attempts 
       of booting dev1 without recovering lan, first.  I fixed it already. Everybody did 
       a good job! Thanks.

  <LI> <a href="http://amdm/LectureNotes/Diaries/VirTech-2017.html#VirtIo" 
       target="_b"><b>virtio, vhost</b><a> and <b> 
       <a href="http://amdm/LectureNotes/Diaries/VirTech-2017.html#Plane9" 
       target="_b">9p Filesystem<a></b>

  <UL>
    <LI> Basic Feature
<PRE>
$ diff /src3/KVM/bin/start-gv-10 /src3/KVM/bin/start-gv-10-Virtio
11a12,20
> ##################################################################
> # Turn on virtio and vhost business.
> ##################################################################
> sudo modprobe vhost 
> sudo modprobe vhost-net
> sudo modprobe virtio 
> sudo chmod 666 /dev/vhost-net
> ##################################################################
> 
21c30
< vde_switch<u> -tap tap10</u> -mod 644 -sock=/src3/KVM/network-9616 -mgmt /src3/KVM/network-9616/vde_switch.mgmt -daemon </dev/null >/dev/null
---
> vde_switch -mod 644 -sock=/src3/KVM/network-9616 -mgmt /src3/KVM/network-9616/vde_switch.mgmt -daemon </dev/null >/dev/null
23c32
< kvm -net vde<u>,vlan=0</u>,sock=/src3/KVM/network-9616 -net nic<u>,vlan=0</u>,macaddr=1c:6f:65:c4:8c:40 -m 512M -monitor unix:/src3/KVM/network-9616/MonSock,server,nowait <u>-hda ../Resize/Deb-Virtio.img</u> &
---
> kvm -net vde,sock=/src3/KVM/network-9616 -net nic<u>,model=virtio,netdev=tap10</u>,macaddr=1c:6f:65:c4:8c:40<u> -netdev tap,id=tap10,ifname=tap10,script=no,vhost=on</u> -m 512M -monitor unix:/src3/KVM/network-9616/MonSock,server,nowait<u> -drive file=../Resize/Deb-Virtio.img,if=virtio</u> &
</PRE>

    <LI> <B><CODE>-hdb</CODE></B>

<P> Using this option we may export the entire hard disk to VM, if necessary.  It is 
    convenient, but dangerous.

    <LI> <a href="http://amdm/LinuxRef/Network/NetworkAndRouting.html#9pFSSharing" 
         target="_b">9p Filesystem</a>

<P> We can precisely control which (host) filesystem or directory to be shared with 
    VMs. In UML, we use the <CODE>-hostfs</CODE> (host filesystem) option to share 
    the host's <CODE>/usr/local</CODE> and <CODE>/src2<CODE> filesystems with VMs.
    <CODE>/usr/local</CODE> contains all our locally compiled or developed software 
    packages.  We even can export them in <b>readonly</b> mode.

  </UL>
</OL>